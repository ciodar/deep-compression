{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from scipy.stats import norm\n",
    "\n",
    "from utils import plot_sparsity_matrix,weight_histograms,set_all_seeds, plot_weight_histograms, make_paths_relative_to_root\n",
    "\n",
    "import data as module_data\n",
    "import models as module_arch\n",
    "import evaluation as module_metric\n",
    "\n",
    "import quantization as module_quantize\n",
    "import pruning as module_prune\n",
    "\n",
    "from parse_config import ConfigParser\n",
    "from trainer.trainer import Trainer\n",
    "\n",
    "make_paths_relative_to_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Retrieve a configuration\n",
    "config_dict = {'resume':'runs/models/Mnist_LeNet5/baseline/checkpoint-epoch15.pth'}\n",
    "\n",
    "config = ConfigParser.from_dict(config_dict)\n",
    "\n",
    "logger = config.get_logger('train')\n",
    "\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "model.load_state_dict(torch.load(config.resume)['state_dict'])\n",
    "logger.info(model)\n",
    "\n",
    "# prepare for (multi-device) GPU training\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# get function handles of loss and metrics\n",
    "criterion = getattr(F, config['loss'])\n",
    "metrics = [getattr(module_metric, met) for met in config['metrics']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pruning vs sparsity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sensitivity of conv1.weight to l1_unstructured [10.00% sparsity]\n",
      "Pruning conv1.weight with amount 0.10\n",
      "Pruned 15 weights (90.00% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.1 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [21.25% sparsity]\n",
      "Pruning conv1.weight with amount 0.21\n",
      "Pruned 44 weights (70.67% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.2 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [32.50% sparsity]\n",
      "Pruning conv1.weight with amount 0.33\n",
      "Pruned 78 weights (48.00% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.3 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [43.75% sparsity]\n",
      "Pruning conv1.weight with amount 0.44\n",
      "Pruned 110 weights (26.67% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.4 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [55.00% sparsity]\n",
      "Pruning conv1.weight with amount 0.55\n",
      "Pruned 132 weights (12.00% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.6 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [66.25% sparsity]\n",
      "Pruning conv1.weight with amount 0.66\n",
      "Pruned 144 weights (4.00% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.7 ] | acc@1:0.97 | acc@5:1.00\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [77.50% sparsity]\n",
      "Pruning conv1.weight with amount 0.78\n",
      "Pruned 149 weights (0.67% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.8 ] | acc@1:0.93 | acc@5:1.00\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [88.75% sparsity]\n",
      "Pruning conv1.weight with amount 0.89\n",
      "Pruned 150 weights (0.00% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:0.9 ] | acc@1:0.09 | acc@5:0.49\n",
      "Testing sensitivity of conv1.weight to l1_unstructured [100.00% sparsity]\n",
      "Pruning conv1.weight with amount 1.00\n",
      "Pruned 150 weights (0.00% retained)\n",
      "Tested sensitivity of conv1.weight [ sparsity amount:1.0 ] | acc@1:0.09 | acc@5:0.49\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [10.00% sparsity]\n",
      "Pruning conv2.weight with amount 0.10\n",
      "Pruned 240 weights (90.00% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.1 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [21.25% sparsity]\n",
      "Pruning conv2.weight with amount 0.21\n",
      "Pruned 699 weights (70.88% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.2 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [32.50% sparsity]\n",
      "Pruning conv2.weight with amount 0.33\n",
      "Pruned 1252 weights (47.83% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.3 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [43.75% sparsity]\n",
      "Pruning conv2.weight with amount 0.44\n",
      "Pruned 1754 weights (26.92% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.4 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [55.00% sparsity]\n",
      "Pruning conv2.weight with amount 0.55\n",
      "Pruned 2109 weights (12.12% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.6 ] | acc@1:0.97 | acc@5:1.00\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [66.25% sparsity]\n",
      "Pruning conv2.weight with amount 0.66\n",
      "Pruned 2302 weights (4.08% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.7 ] | acc@1:0.72 | acc@5:0.99\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [77.50% sparsity]\n",
      "Pruning conv2.weight with amount 0.78\n",
      "Pruned 2378 weights (0.92% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.8 ] | acc@1:0.38 | acc@5:0.84\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [88.75% sparsity]\n",
      "Pruning conv2.weight with amount 0.89\n",
      "Pruned 2398 weights (0.08% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:0.9 ] | acc@1:0.16 | acc@5:0.67\n",
      "Testing sensitivity of conv2.weight to l1_unstructured [100.00% sparsity]\n",
      "Pruning conv2.weight with amount 1.00\n",
      "Pruned 2400 weights (0.00% retained)\n",
      "Tested sensitivity of conv2.weight [ sparsity amount:1.0 ] | acc@1:0.09 | acc@5:0.48\n",
      "Testing sensitivity of fc.weight to l1_unstructured [10.00% sparsity]\n",
      "Pruning fc.weight with amount 0.10\n",
      "Pruned 4800 weights (90.00% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.1 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc.weight to l1_unstructured [21.25% sparsity]\n",
      "Pruning fc.weight with amount 0.21\n",
      "Pruned 13980 weights (70.88% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.2 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc.weight to l1_unstructured [32.50% sparsity]\n",
      "Pruning fc.weight with amount 0.33\n",
      "Pruned 25036 weights (47.84% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.3 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc.weight to l1_unstructured [43.75% sparsity]\n",
      "Pruning fc.weight with amount 0.44\n",
      "Pruned 35083 weights (26.91% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.4 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc.weight to l1_unstructured [55.00% sparsity]\n",
      "Pruning fc.weight with amount 0.55\n",
      "Pruned 42187 weights (12.11% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.6 ] | acc@1:0.99 | acc@5:1.00\n",
      "Testing sensitivity of fc.weight to l1_unstructured [66.25% sparsity]\n",
      "Pruning fc.weight with amount 0.66\n",
      "Pruned 46038 weights (4.09% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.7 ] | acc@1:0.96 | acc@5:1.00\n",
      "Testing sensitivity of fc.weight to l1_unstructured [77.50% sparsity]\n",
      "Pruning fc.weight with amount 0.78\n",
      "Pruned 47559 weights (0.92% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.8 ] | acc@1:0.76 | acc@5:0.99\n",
      "Testing sensitivity of fc.weight to l1_unstructured [88.75% sparsity]\n",
      "Pruning fc.weight with amount 0.89\n",
      "Pruned 47950 weights (0.10% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:0.9 ] | acc@1:0.24 | acc@5:0.75\n",
      "Testing sensitivity of fc.weight to l1_unstructured [100.00% sparsity]\n",
      "Pruning fc.weight with amount 1.00\n",
      "Pruned 48000 weights (0.00% retained)\n",
      "Tested sensitivity of fc.weight [ sparsity amount:1.0 ] | acc@1:0.09 | acc@5:0.49\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [10.00% sparsity]\n",
      "Pruning fc1.weight with amount 0.10\n",
      "Pruned 1008 weights (90.00% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.1 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [21.25% sparsity]\n",
      "Pruning fc1.weight with amount 0.21\n",
      "Pruned 2936 weights (70.87% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.2 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [32.50% sparsity]\n",
      "Pruning fc1.weight with amount 0.33\n",
      "Pruned 5258 weights (47.84% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.3 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [43.75% sparsity]\n",
      "Pruning fc1.weight with amount 0.44\n",
      "Pruned 7368 weights (26.90% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.4 ] | acc@1:0.99 | acc@5:1.00\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [55.00% sparsity]\n",
      "Pruning fc1.weight with amount 0.55\n",
      "Pruned 8860 weights (12.10% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.6 ] | acc@1:0.99 | acc@5:1.00\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [66.25% sparsity]\n",
      "Pruning fc1.weight with amount 0.66\n",
      "Pruned 9668 weights (4.09% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.7 ] | acc@1:0.94 | acc@5:1.00\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [77.50% sparsity]\n",
      "Pruning fc1.weight with amount 0.78\n",
      "Pruned 9987 weights (0.92% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.8 ] | acc@1:0.70 | acc@5:0.98\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [88.75% sparsity]\n",
      "Pruning fc1.weight with amount 0.89\n",
      "Pruned 10070 weights (0.10% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:0.9 ] | acc@1:0.29 | acc@5:0.74\n",
      "Testing sensitivity of fc1.weight to l1_unstructured [100.00% sparsity]\n",
      "Pruning fc1.weight with amount 1.00\n",
      "Pruned 10080 weights (0.00% retained)\n",
      "Tested sensitivity of fc1.weight [ sparsity amount:1.0 ] | acc@1:0.10 | acc@5:0.49\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [10.00% sparsity]\n",
      "Pruning fc2.weight with amount 0.10\n",
      "Pruned 84 weights (90.00% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.1 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [21.25% sparsity]\n",
      "Pruning fc2.weight with amount 0.21\n",
      "Pruned 245 weights (70.83% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.2 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [32.50% sparsity]\n",
      "Pruning fc2.weight with amount 0.33\n",
      "Pruned 438 weights (47.86% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.3 ] | acc@1:1.00 | acc@5:1.00\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [43.75% sparsity]\n",
      "Pruning fc2.weight with amount 0.44\n",
      "Pruned 614 weights (26.90% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.4 ] | acc@1:0.99 | acc@5:1.00\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [55.00% sparsity]\n",
      "Pruning fc2.weight with amount 0.55\n",
      "Pruned 738 weights (12.14% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.6 ] | acc@1:0.97 | acc@5:1.00\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [66.25% sparsity]\n",
      "Pruning fc2.weight with amount 0.66\n",
      "Pruned 806 weights (4.05% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.7 ] | acc@1:0.67 | acc@5:0.89\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [77.50% sparsity]\n",
      "Pruning fc2.weight with amount 0.78\n",
      "Pruned 832 weights (0.95% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.8 ] | acc@1:0.40 | acc@5:0.84\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [88.75% sparsity]\n",
      "Pruning fc2.weight with amount 0.89\n",
      "Pruned 839 weights (0.12% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:0.9 ] | acc@1:0.17 | acc@5:0.58\n",
      "Testing sensitivity of fc2.weight to l1_unstructured [100.00% sparsity]\n",
      "Pruning fc2.weight with amount 1.00\n",
      "Pruned 840 weights (0.00% retained)\n",
      "Tested sensitivity of fc2.weight [ sparsity amount:1.0 ] | acc@1:0.11 | acc@5:0.50\n"
     ]
    }
   ],
   "source": [
    "from pruning import prune_model\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "sparsities = np.linspace(.1,1,9)\n",
    "train = False\n",
    "\n",
    "sensitivities = OrderedDict()\n",
    "for pruner in config['pruners']:\n",
    "    for param_name in pruner['levels'].keys():\n",
    "        if model.state_dict()[param_name].dim() not in [2, 4]:\n",
    "            continue\n",
    "\n",
    "        # Make a copy of the model, because when we apply the zeros mask (i.e.\n",
    "        # perform pruning), the model's weights are altered\n",
    "        model_cpy = deepcopy(model)\n",
    "\n",
    "        # build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler\n",
    "        trainable_params = filter(lambda p: p.requires_grad, model_cpy.parameters())\n",
    "        optimizer = config.init_obj('optimizer', torch.optim, trainable_params)\n",
    "        lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)\n",
    "\n",
    "        trainer = Trainer(model_cpy, criterion, metrics, optimizer,\n",
    "                          config=config,\n",
    "                          device=device,\n",
    "                          data_loader=data_loader,\n",
    "                          valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=lr_scheduler)\n",
    "\n",
    "        sensitivity = OrderedDict()\n",
    "        for l in sparsities:\n",
    "            sparsity = float(l)\n",
    "            # Element-wise sparsity\n",
    "            levels = {param_name: sparsity}\n",
    "\n",
    "            fn = getattr(module_prune, pruner[\"type\"])\n",
    "            if logger is not None:\n",
    "                logger.info(\"Testing sensitivity of {} to {} [{:.2f}% sparsity]\".format(\n",
    "                    param_name,fn.__name__, sparsity * 100))\n",
    "            # Create the pruner (a level pruner), the pruning policy and the\n",
    "            # pruning schedule.\n",
    "\n",
    "\n",
    "            iterations = 1\n",
    "            for it in range(iterations):\n",
    "                prune_model(model_cpy, fn, levels, logger)\n",
    "\n",
    "                # Test and record the performance of the pruned model\n",
    "                if train:\n",
    "                    trainer.train()\n",
    "            _, acc1, acc5 = trainer._valid_epoch(-1).values()\n",
    "            if logger is not None:\n",
    "                logger.info(\n",
    "                    \"Tested sensitivity of {} [ sparsity amount:{:.1f} ] | acc@1:{:.2f} | acc@5:{:.2f}\"\n",
    "                    .format(param_name,\n",
    "                            sparsity,\n",
    "                            acc1, acc5))\n",
    "            sensitivity[sparsity] = [acc1, acc5]\n",
    "        sensitivities[param_name] = sensitivity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.998314019756839, 0.9983377659574468, 0.9986702127659575, 0.996841755319149, 0.9954882218844985, 0.9676339285714286, 0.933843085106383, 0.0928951367781155, 0.0928238981762918] [0.1, 0.21250000000000002, 0.325, 0.4375, 0.55, 0.6625, 0.775, 0.8875, 1.0]\n",
      "[0.9983377659574468, 0.9981715425531915, 0.9986464665653496, 0.996343085106383, 0.9676814209726443, 0.7234517477203648, 0.37955927051671734, 0.15874335106382978, 0.09277640577507598] [0.1, 0.21250000000000002, 0.325, 0.4375, 0.55, 0.6625, 0.775, 0.8875, 1.0]\n",
      "[0.9983377659574468, 0.997815349544073, 0.9975066489361702, 0.9951795212765957, 0.9867021276595744, 0.9585153875379939, 0.761231952887538, 0.24411094224924013, 0.09280015197568389] [0.1, 0.21250000000000002, 0.325, 0.4375, 0.55, 0.6625, 0.775, 0.8875, 1.0]\n",
      "[0.9983377659574468, 0.9980053191489362, 0.9973404255319149, 0.9940159574468085, 0.9881743920972644, 0.9436502659574468, 0.6968322568389057, 0.29212575987841943, 0.1015862462006079] [0.1, 0.21250000000000002, 0.325, 0.4375, 0.55, 0.6625, 0.775, 0.8875, 1.0]\n",
      "[0.9983377659574468, 0.9981477963525837, 0.9973404255319149, 0.9866546352583586, 0.9732855243161095, 0.666033434650456, 0.4028068009118541, 0.17213620820668696, 0.10795022796352584] [0.1, 0.21250000000000002, 0.325, 0.4375, 0.55, 0.6625, 0.775, 0.8875, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def sensitivities_to_csv(sensitivities, fname):\n",
    "    \"\"\"Create a CSV file listing from the sensitivities dictionary.\n",
    "    The 'sensitivities' argument is expected to have the dict-of-dict structure\n",
    "    described in the documentation of perform_sensitivity_test.\n",
    "    \"\"\"\n",
    "    with open(fname, 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        # write the header\n",
    "        writer.writerow(['parameter', 'sparsity', 'top1', 'top5', 'loss'])\n",
    "        for param_name, sensitivity in sensitivities.items():\n",
    "            for sparsity, values in sensitivity.items():\n",
    "                writer.writerow([param_name] + [sparsity] + list(values))\n",
    "\n",
    "\n",
    "try:\n",
    "    # sudo apt-get install python3-tk\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"WARNING: Function plot_sensitivity requires package matplotlib which\"\n",
    "          \"is not installed in your execution environment.\\n\"\n",
    "          \"Skipping the PNG file generation\")\n",
    "\n",
    "fig = plt.figure()\n",
    "for param_name, sensitivity in sensitivities.items():\n",
    "    sense = [values[0] for sparsity, values in sensitivity.items()]\n",
    "    sparsities = [sparsity for sparsity, values in sensitivity.items()]\n",
    "    print(sense,sparsities)\n",
    "    plt.plot(sparsities, sense, label=param_name)\n",
    "\n",
    "plt.ylabel('top1 Accuracy')\n",
    "plt.xlabel('sparsity')\n",
    "plt.title('Pruning Sensitivity')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left',\n",
    "           ncol=2, borderaxespad=0.)\n",
    "fig.savefig('pruning-sensitivity-analysis-notrain.png')\n",
    "sensitivities_to_csv(sensitivities,'pruning-sensitivity-analysis-notrain.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quantization vs bits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: runs\\models\\Mnist_LeNet5\\baseline\\checkpoint-epoch15.pth ...\n",
      "Checkpoint loaded. Resume training from epoch 16\n",
      "Accuracy before compression: 0.9903\n",
      "   Pruned 51 weights - 99 (66.00%) retained)\n",
      "   Pruned 2112 weights - 288 (12.00%) retained)\n",
      "    epoch          : 1\n",
      "    loss           : 0.025489811674069385\n",
      "    accuracy       : 0.9927957853757617\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.0243437032443174\n",
      "    val_accuracy   : 0.9911426671732522\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.019787589820505724\n",
      "    accuracy       : 0.9942424466824644\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.020903266210881795\n",
      "    val_accuracy   : 0.9938497340425532\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.018551391798101356\n",
      "    accuracy       : 0.9945756812796208\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.019917802077698263\n",
      "    val_accuracy   : 0.9934935410334347\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.01762111214490528\n",
      "    accuracy       : 0.9949274289099526\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.01867669444293418\n",
      "    val_accuracy   : 0.9938022416413373\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.017098626193189904\n",
      "    accuracy       : 0.9951310722748815\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.0178693969451644\n",
      "    val_accuracy   : 0.9939684650455927\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.016540152291652538\n",
      "    accuracy       : 0.9950914014895058\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.018165601585852974\n",
      "    val_accuracy   : 0.9946808510638298\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.01637518492901656\n",
      "    accuracy       : 0.9950358623899798\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.01682744508341668\n",
      "    val_accuracy   : 0.9946571048632219\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.016091366066217812\n",
      "    accuracy       : 0.9952209927217333\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.017225605014592728\n",
      "    val_accuracy   : 0.9950132978723404\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.015784342583738956\n",
      "    accuracy       : 0.9952236374407583\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.016912891056408433\n",
      "    val_accuracy   : 0.9948470744680851\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.015599569067705458\n",
      "    accuracy       : 0.9954272808056872\n",
      "    topk_accuracy  : 0.9999814869668247\n",
      "    val_loss       : 0.016750345583707887\n",
      "    val_accuracy   : 0.9949895516717325\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Pruning iteration 1 | acc@1:0.9950 | acc@5:1.0000\n",
      "   Pruned 44160 weights - 3840 (8.00%) retained)\n",
      "   Pruned 9274 weights - 806 (8.00%) retained)\n",
      "   Pruned 680 weights - 160 (19.05%) retained)\n",
      "    epoch          : 1\n",
      "    loss           : 0.07107765288688066\n",
      "    accuracy       : 0.9815213481719702\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.05349573758529856\n",
      "    val_accuracy   : 0.9823090805471124\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 2\n",
      "    loss           : 0.04364824211842853\n",
      "    accuracy       : 0.9870170743060257\n",
      "    topk_accuracy  : 0.9998518957345972\n",
      "    val_loss       : 0.04856838125418475\n",
      "    val_accuracy   : 0.9841612841945289\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.037434877300860085\n",
      "    accuracy       : 0.9887811018957346\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.04269778437199111\n",
      "    val_accuracy   : 0.986345934650456\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.03348538975045968\n",
      "    accuracy       : 0.9899103969194313\n",
      "    topk_accuracy  : 0.9998518957345972\n",
      "    val_loss       : 0.0379041045448406\n",
      "    val_accuracy   : 0.9868208586626139\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.03079387305251442\n",
      "    accuracy       : 0.9908122461069735\n",
      "    topk_accuracy  : 0.9999259478672986\n",
      "    val_loss       : 0.0356524930375212\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.028678851327925074\n",
      "    accuracy       : 0.9914443339539608\n",
      "    topk_accuracy  : 0.9999074348341233\n",
      "    val_loss       : 0.03695890721072383\n",
      "    val_accuracy   : 0.9876519756838905\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.026804472870409702\n",
      "    accuracy       : 0.9917987263033176\n",
      "    topk_accuracy  : 0.9999259478672986\n",
      "    val_loss       : 0.03614900644591197\n",
      "    val_accuracy   : 0.9888155395136777\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.025845399756487725\n",
      "    accuracy       : 0.9923303148273527\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0381412254250113\n",
      "    val_accuracy   : 0.9888155395136777\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.024526609352820715\n",
      "    accuracy       : 0.9925868525727827\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03557149796905194\n",
      "    val_accuracy   : 0.9869870820668692\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.023339789132896582\n",
      "    accuracy       : 0.993020586492891\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03387664975796608\n",
      "    val_accuracy   : 0.9881981382978723\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Pruning iteration 1 | acc@1:0.9882 | acc@5:1.0000\n",
      "Total pruned: 56277 | Retained: 5193 (8%) | Compression factor: 12X \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.32038171857736686\n",
      "    accuracy       : 0.918592903689912\n",
      "    topk_accuracy  : 0.9984449052132701\n",
      "    val_loss       : 0.2534989131257889\n",
      "    val_accuracy   : 0.9453837386018237\n",
      "    val_topk_accuracy: 0.9991688829787234\n",
      "    epoch          : 2\n",
      "    loss           : 0.24735908150249183\n",
      "    accuracy       : 0.9429560553486797\n",
      "    topk_accuracy  : 0.9989632701421801\n",
      "    val_loss       : 0.23911034014630825\n",
      "    val_accuracy   : 0.9464523176291794\n",
      "    val_topk_accuracy: 0.9991688829787234\n",
      "    epoch          : 3\n",
      "    loss           : 0.23665282338575164\n",
      "    accuracy       : 0.9440271665538253\n",
      "    topk_accuracy  : 0.9990002962085308\n",
      "    val_loss       : 0.22995140165724653\n",
      "    val_accuracy   : 0.947045972644377\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 4\n",
      "    loss           : 0.22854821002596362\n",
      "    accuracy       : 0.9453865521327014\n",
      "    topk_accuracy  : 0.9990928613744076\n",
      "    val_loss       : 0.22298390751189373\n",
      "    val_accuracy   : 0.948518237082067\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 5\n",
      "    loss           : 0.22194499434170564\n",
      "    accuracy       : 0.9467988320920786\n",
      "    topk_accuracy  : 0.9991298874407583\n",
      "    val_loss       : 0.21729588603719752\n",
      "    val_accuracy   : 0.9496343085106383\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.21642472884525055\n",
      "    accuracy       : 0.9478329172308734\n",
      "    topk_accuracy  : 0.9991484004739336\n",
      "    val_loss       : 0.21230771059685566\n",
      "    val_accuracy   : 0.9507028875379939\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 7\n",
      "    loss           : 0.21170912315777693\n",
      "    accuracy       : 0.9486818720379147\n",
      "    topk_accuracy  : 0.9991484004739336\n",
      "    val_loss       : 0.2078995479548231\n",
      "    val_accuracy   : 0.9509878419452887\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 8\n",
      "    loss           : 0.2076576937749205\n",
      "    accuracy       : 0.9490441985443466\n",
      "    topk_accuracy  : 0.9991272427217333\n",
      "    val_loss       : 0.20430436413338843\n",
      "    val_accuracy   : 0.9520326747720365\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 9\n",
      "    loss           : 0.20402968832031246\n",
      "    accuracy       : 0.9498111670616114\n",
      "    topk_accuracy  : 0.9990928613744076\n",
      "    val_loss       : 0.2011667883142512\n",
      "    val_accuracy   : 0.9528875379939209\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 10\n",
      "    loss           : 0.20082902657618454\n",
      "    accuracy       : 0.9502951506431956\n",
      "    topk_accuracy  : 0.999166913507109\n",
      "    val_loss       : 0.19819697515761597\n",
      "    val_accuracy   : 0.9532912234042553\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Tested model after quantization - acc@1:0.9533 | acc@5:0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.1115192914717076\n",
      "    accuracy       : 0.9655789818889641\n",
      "    topk_accuracy  : 0.9997037914691943\n",
      "    val_loss       : 0.08241929704363042\n",
      "    val_accuracy   : 0.9747815349544073\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 2\n",
      "    loss           : 0.06858365911487226\n",
      "    accuracy       : 0.978593644211239\n",
      "    topk_accuracy  : 0.9998333827014217\n",
      "    val_loss       : 0.06618348584371678\n",
      "    val_accuracy   : 0.9789846124620061\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 3\n",
      "    loss           : 0.05862537426079548\n",
      "    accuracy       : 0.9812595209884901\n",
      "    topk_accuracy  : 0.9998333827014217\n",
      "    val_loss       : 0.060744993547175795\n",
      "    val_accuracy   : 0.981216755319149\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 4\n",
      "    loss           : 0.05487050986252902\n",
      "    accuracy       : 0.9825898146580907\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.05858863212485263\n",
      "    val_accuracy   : 0.9820478723404256\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 5\n",
      "    loss           : 0.053133803211214345\n",
      "    accuracy       : 0.9830976007109005\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.05764617037741428\n",
      "    val_accuracy   : 0.9824753039513677\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.05225235386089515\n",
      "    accuracy       : 0.9835577818212593\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.05708800228510765\n",
      "    val_accuracy   : 0.982641527355623\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 7\n",
      "    loss           : 0.051740475528613085\n",
      "    accuracy       : 0.9835366240690588\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.05677073631197848\n",
      "    val_accuracy   : 0.9828314969604862\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 8\n",
      "    loss           : 0.05140595960373421\n",
      "    accuracy       : 0.9835551371022342\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.05656721734223848\n",
      "    val_accuracy   : 0.9829977203647415\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 9\n",
      "    loss           : 0.05113763516212668\n",
      "    accuracy       : 0.9836529917061612\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.0562633155706398\n",
      "    val_accuracy   : 0.9828552431610943\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 10\n",
      "    loss           : 0.050944257287034916\n",
      "    accuracy       : 0.9837032413676371\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.05621595884812005\n",
      "    val_accuracy   : 0.9826652735562309\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Tested model after quantization - acc@1:0.9825 | acc@5:0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.02937035209783993\n",
      "    accuracy       : 0.9908545616113744\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03872157923599824\n",
      "    val_accuracy   : 0.9870108282674772\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.027984983580996905\n",
      "    accuracy       : 0.9911481254231551\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03759136489850092\n",
      "    val_accuracy   : 0.9876994680851063\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.027166088923571356\n",
      "    accuracy       : 0.9914073078876101\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0369915543272695\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.026667322109396\n",
      "    accuracy       : 0.9917035164184157\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03673382184686179\n",
      "    val_accuracy   : 0.9884593465045592\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.026357784689353313\n",
      "    accuracy       : 0.9918172393364929\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.036574423283417805\n",
      "    val_accuracy   : 0.9882931231003039\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.026155983657306897\n",
      "    accuracy       : 0.991983856635071\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.036324247257149916\n",
      "    val_accuracy   : 0.9882931231003039\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.02603497981950111\n",
      "    accuracy       : 0.9920155932633716\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.036136433482170105\n",
      "    val_accuracy   : 0.988530585106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.02593563179545087\n",
      "    accuracy       : 0.9920579087677726\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0360856332043384\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.025876023810217818\n",
      "    accuracy       : 0.99192567281652\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03610169554644443\n",
      "    val_accuracy   : 0.9886493161094224\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.025844188180301885\n",
      "    accuracy       : 0.9919388964116452\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03612416474386416\n",
      "    val_accuracy   : 0.988981762917933\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Tested model after quantization - acc@1:0.9890 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.022363600843051952\n",
      "    accuracy       : 0.9934252285037238\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03437427127178996\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.021774443878319946\n",
      "    accuracy       : 0.993650029620853\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03396842467717509\n",
      "    val_accuracy   : 0.988981762917933\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.021399523077479683\n",
      "    accuracy       : 0.9938140022004063\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03379367218610454\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.02114466820033576\n",
      "    accuracy       : 0.9938351599526066\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03376737832923994\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.020970116276459512\n",
      "    accuracy       : 0.9937954891672309\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033617138015502625\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.020845827246548265\n",
      "    accuracy       : 0.993909212085308\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03362444036224104\n",
      "    val_accuracy   : 0.9891479863221884\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.020762563206120813\n",
      "    accuracy       : 0.9938906990521327\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03366997280960625\n",
      "    val_accuracy   : 0.9889580167173252\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.020699072006860334\n",
      "    accuracy       : 0.9938880543331077\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033569350878291944\n",
      "    val_accuracy   : 0.9891717325227964\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.020645711574840284\n",
      "    accuracy       : 0.9939277251184834\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03363883538676941\n",
      "    val_accuracy   : 0.9891717325227964\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.020608258068393826\n",
      "    accuracy       : 0.9939462381516587\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03367486138372345\n",
      "    val_accuracy   : 0.9891479863221884\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Tested model after quantization - acc@1:0.9892 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.02049147462614666\n",
      "    accuracy       : 0.9940388033175356\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03463407106291344\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.020277955696225095\n",
      "    accuracy       : 0.9941260790453622\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.034392545940948926\n",
      "    val_accuracy   : 0.9891479863221884\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.020107919476061213\n",
      "    accuracy       : 0.9943535248815166\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.034250541084862134\n",
      "    val_accuracy   : 0.9892904635258358\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.01999651455299452\n",
      "    accuracy       : 0.9944222875761678\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03411607492338628\n",
      "    val_accuracy   : 0.9896704027355624\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.01990780280185731\n",
      "    accuracy       : 0.9944804713947191\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03398625192509845\n",
      "    val_accuracy   : 0.9895279255319149\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.019831530396689737\n",
      "    accuracy       : 0.9945201421800948\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.034050451512349415\n",
      "    val_accuracy   : 0.989480433130699\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.019780201718798216\n",
      "    accuracy       : 0.9945360104942451\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0340123528842517\n",
      "    val_accuracy   : 0.9894566869300911\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.019737840556182072\n",
      "    accuracy       : 0.9945201421800948\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03397941364946359\n",
      "    val_accuracy   : 0.9893142097264437\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.019717954766221558\n",
      "    accuracy       : 0.994475181956669\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03389659995569828\n",
      "    val_accuracy   : 0.9891717325227964\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.01968073834771604\n",
      "    accuracy       : 0.9944831161137441\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03410246571218793\n",
      "    val_accuracy   : 0.9890767477203648\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9891 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.019646573489006627\n",
      "    accuracy       : 0.9944434453283684\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03372066079371391\n",
      "    val_accuracy   : 0.9888155395136777\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.019543525797063352\n",
      "    accuracy       : 0.9944408006093433\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03367810979227912\n",
      "    val_accuracy   : 0.9888155395136777\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.019457072398064754\n",
      "    accuracy       : 0.9944646030805687\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033561679603967894\n",
      "    val_accuracy   : 0.9887917933130699\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.019401028058248886\n",
      "    accuracy       : 0.9944619583615437\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03347681091721863\n",
      "    val_accuracy   : 0.9886968085106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.019352459512050682\n",
      "    accuracy       : 0.9944831161137441\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03348772700856182\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.019322056549499738\n",
      "    accuracy       : 0.9945545235274205\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03346012419763398\n",
      "    val_accuracy   : 0.989029255319149\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.019292385039864312\n",
      "    accuracy       : 0.9944646030805687\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03349976555662269\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.01928164678133618\n",
      "    accuracy       : 0.9944989844278944\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03346526608406071\n",
      "    val_accuracy   : 0.989029255319149\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.019268109140116545\n",
      "    accuracy       : 0.994477826675694\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0335384362198888\n",
      "    val_accuracy   : 0.9890055091185411\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.019241182174383534\n",
      "    accuracy       : 0.994427577014218\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033492807554021634\n",
      "    val_accuracy   : 0.9891954787234043\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Tested model after quantization - acc@1:0.9892 | acc@5:1.0000\n",
      "Warning: number of elements 99 is less than number of clusters. using 6 bits for quantization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.019872561243422862\n",
      "    accuracy       : 0.9943482354434664\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033816584693069786\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.0196907677245216\n",
      "    accuracy       : 0.9944646030805687\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03375327880147527\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.01957347993137562\n",
      "    accuracy       : 0.9945174974610698\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03375472124428191\n",
      "    val_accuracy   : 0.9886493161094224\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.01948051457651957\n",
      "    accuracy       : 0.9945941943127962\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033578581325313515\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.019425674987696463\n",
      "    accuracy       : 0.99452807633717\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03366296937035278\n",
      "    val_accuracy   : 0.9886493161094224\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.019360375020132176\n",
      "    accuracy       : 0.9944831161137441\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03353819847503241\n",
      "    val_accuracy   : 0.988530585106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.019345684188183054\n",
      "    accuracy       : 0.9944593136425186\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03363361570270772\n",
      "    val_accuracy   : 0.9884830927051671\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.01928966842488037\n",
      "    accuracy       : 0.9944460900473934\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03375669788975427\n",
      "    val_accuracy   : 0.9884830927051671\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.019267267364097132\n",
      "    accuracy       : 0.994427577014218\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03359285441346149\n",
      "    val_accuracy   : 0.9883406155015197\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.019248479948008337\n",
      "    accuracy       : 0.9944646030805687\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03369179784141956\n",
      "    val_accuracy   : 0.9883168693009118\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9883 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.32007205270901673\n",
      "    accuracy       : 0.9190054798578199\n",
      "    topk_accuracy  : 0.998537470379147\n",
      "    val_loss       : 0.25376454724910413\n",
      "    val_accuracy   : 0.9448375759878419\n",
      "    val_topk_accuracy: 0.9990026595744681\n",
      "    epoch          : 2\n",
      "    loss           : 0.2472901923396576\n",
      "    accuracy       : 0.9429216740013542\n",
      "    topk_accuracy  : 0.9990373222748815\n",
      "    val_loss       : 0.23898474428247898\n",
      "    val_accuracy   : 0.9461911094224925\n",
      "    val_topk_accuracy: 0.9991688829787234\n",
      "    epoch          : 3\n",
      "    loss           : 0.23664419956823096\n",
      "    accuracy       : 0.9442149416046038\n",
      "    topk_accuracy  : 0.9990373222748815\n",
      "    val_loss       : 0.22999772112420264\n",
      "    val_accuracy   : 0.9474021656534954\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 4\n",
      "    loss           : 0.2285552334163991\n",
      "    accuracy       : 0.9454262229180771\n",
      "    topk_accuracy  : 0.9990743483412322\n",
      "    val_loss       : 0.22313035675819884\n",
      "    val_accuracy   : 0.9483995060790273\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 5\n",
      "    loss           : 0.22191642071186649\n",
      "    accuracy       : 0.9466560172647258\n",
      "    topk_accuracy  : 0.9991113744075829\n",
      "    val_loss       : 0.21708576159274323\n",
      "    val_accuracy   : 0.9501567249240122\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.21641228718780228\n",
      "    accuracy       : 0.9478884563303994\n",
      "    topk_accuracy  : 0.999166913507109\n",
      "    val_loss       : 0.21215727196094838\n",
      "    val_accuracy   : 0.9510115881458967\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 7\n",
      "    loss           : 0.21169091817609506\n",
      "    accuracy       : 0.9486527801286392\n",
      "    topk_accuracy  : 0.9991484004739336\n",
      "    val_loss       : 0.20793828621823737\n",
      "    val_accuracy   : 0.9512965425531915\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 8\n",
      "    loss           : 0.20762554469619882\n",
      "    accuracy       : 0.9493536306702777\n",
      "    topk_accuracy  : 0.9991113744075829\n",
      "    val_loss       : 0.20425654630711737\n",
      "    val_accuracy   : 0.9517239741641337\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 9\n",
      "    loss           : 0.2040242912397848\n",
      "    accuracy       : 0.9497662068381855\n",
      "    topk_accuracy  : 0.9991298874407583\n",
      "    val_loss       : 0.20099311940213468\n",
      "    val_accuracy   : 0.9529587765957447\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 10\n",
      "    loss           : 0.20081614162684616\n",
      "    accuracy       : 0.9503692027758971\n",
      "    topk_accuracy  : 0.9991484004739336\n",
      "    val_loss       : 0.19820360982037605\n",
      "    val_accuracy   : 0.9535049392097265\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Tested model after quantization - acc@1:0.9535 | acc@5:0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.11174890114285793\n",
      "    accuracy       : 0.9657112178402167\n",
      "    topk_accuracy  : 0.9996482523696683\n",
      "    val_loss       : 0.08301442766443212\n",
      "    val_accuracy   : 0.9742591185410334\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.069753249341366\n",
      "    accuracy       : 0.9784085138794854\n",
      "    topk_accuracy  : 0.9998333827014217\n",
      "    val_loss       : 0.06809476683748529\n",
      "    val_accuracy   : 0.9793408054711246\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.060420120810268894\n",
      "    accuracy       : 0.9810135621191605\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.0633455236699987\n",
      "    val_accuracy   : 0.979720744680851\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.056962413653749\n",
      "    accuracy       : 0.981981529282329\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.06151652391603653\n",
      "    val_accuracy   : 0.9805281155015197\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.05541674388457814\n",
      "    accuracy       : 0.9823359216316859\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.060666872584756386\n",
      "    val_accuracy   : 0.9813354863221884\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.05460791852793069\n",
      "    accuracy       : 0.9824417103926879\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.060184117049929944\n",
      "    val_accuracy   : 0.9813592325227964\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.054111128898063826\n",
      "    accuracy       : 0.9826294854434664\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.05981266310319622\n",
      "    val_accuracy   : 0.9813829787234043\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.05378716101569799\n",
      "    accuracy       : 0.9827749449898443\n",
      "    topk_accuracy  : 0.9999074348341233\n",
      "    val_loss       : 0.05985415437595641\n",
      "    val_accuracy   : 0.9812879939209727\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.053511599330363965\n",
      "    accuracy       : 0.9829468517264727\n",
      "    topk_accuracy  : 0.9999074348341233\n",
      "    val_loss       : 0.05945673045959878\n",
      "    val_accuracy   : 0.9813592325227964\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.0532928735803442\n",
      "    accuracy       : 0.9829309834123223\n",
      "    topk_accuracy  : 0.9999074348341233\n",
      "    val_loss       : 0.05924755326927977\n",
      "    val_accuracy   : 0.9818816489361702\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Tested model after quantization - acc@1:0.9819 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.03174212506102725\n",
      "    accuracy       : 0.9903705780297901\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.04097252850361327\n",
      "    val_accuracy   : 0.9878181990881458\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.02955822099907262\n",
      "    accuracy       : 0.9912618483412322\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03925547098859827\n",
      "    val_accuracy   : 0.9876757218844985\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.028260923266605037\n",
      "    accuracy       : 0.9916506220379147\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03827588987398021\n",
      "    val_accuracy   : 0.9876282294832827\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.02744998720803373\n",
      "    accuracy       : 0.9919626988828707\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03763199020969741\n",
      "    val_accuracy   : 0.9878419452887538\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.026919259300225932\n",
      "    accuracy       : 0.9922033683141503\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.037260287867343804\n",
      "    val_accuracy   : 0.9880081686930091\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.026558570977039987\n",
      "    accuracy       : 0.992293288761002\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03702809173673233\n",
      "    val_accuracy   : 0.9879844224924011\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.02630890361243038\n",
      "    accuracy       : 0.9924255247122546\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03678263392438121\n",
      "    val_accuracy   : 0.988031914893617\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.02612869207409621\n",
      "    accuracy       : 0.992502221563981\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0367653721775067\n",
      "    val_accuracy   : 0.9880081686930091\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.026012142488988938\n",
      "    accuracy       : 0.9925551159444821\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.036615319966160235\n",
      "    val_accuracy   : 0.9876757218844985\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.02590686701992924\n",
      "    accuracy       : 0.9925207345971564\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03667267481301059\n",
      "    val_accuracy   : 0.9876519756838905\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Tested model after quantization - acc@1:0.9877 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.024222434530921846\n",
      "    accuracy       : 0.9925551159444821\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.038270830548308946\n",
      "    val_accuracy   : 0.9883406155015197\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.023718741101431285\n",
      "    accuracy       : 0.9927984300947867\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03781315727912365\n",
      "    val_accuracy   : 0.9883643617021277\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.02334909839555621\n",
      "    accuracy       : 0.993020586492891\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.037516657510732715\n",
      "    val_accuracy   : 0.9878656914893617\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.023072733278982165\n",
      "    accuracy       : 0.9930761255924171\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03740222567315907\n",
      "    val_accuracy   : 0.9878181990881458\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.0228710413096093\n",
      "    accuracy       : 0.9931290199729181\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03721656473452582\n",
      "    val_accuracy   : 0.9878419452887538\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.022717898308798248\n",
      "    accuracy       : 0.9932030721056195\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.037124115847388006\n",
      "    val_accuracy   : 0.9876757218844985\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.02258468233876513\n",
      "    accuracy       : 0.9932797689573459\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03702632971900575\n",
      "    val_accuracy   : 0.9874857522796352\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.02249171239474537\n",
      "    accuracy       : 0.9932586112051456\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.036924297891636475\n",
      "    val_accuracy   : 0.9876757218844985\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9877 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.02055756918442109\n",
      "    accuracy       : 0.9939991325321598\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03435028626761855\n",
      "    val_accuracy   : 0.9883643617021277\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.02035423156171397\n",
      "    accuracy       : 0.9940017772511849\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03421983658831487\n",
      "    val_accuracy   : 0.988530585106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.020217449357402092\n",
      "    accuracy       : 0.9941049212931617\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.03421819436938522\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.020095187363971215\n",
      "    accuracy       : 0.9941287237643873\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03417467505254961\n",
      "    val_accuracy   : 0.9881506458966565\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.020005407169355317\n",
      "    accuracy       : 0.9942424466824644\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03414878007916219\n",
      "    val_accuracy   : 0.9883406155015197\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.019939928826483152\n",
      "    accuracy       : 0.9942768280297901\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03406977911598663\n",
      "    val_accuracy   : 0.9883643617021277\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.01989115762603756\n",
      "    accuracy       : 0.9942953410629655\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03419573517377548\n",
      "    val_accuracy   : 0.9883168693009118\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.019843204367367365\n",
      "    accuracy       : 0.9943164988151659\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03410978219967256\n",
      "    val_accuracy   : 0.9881743920972644\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9882 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.02065072162518216\n",
      "    accuracy       : 0.993755818381855\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03470046960431686\n",
      "    val_accuracy   : 0.9881743920972644\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.020335138793063673\n",
      "    accuracy       : 0.9938166469194313\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03441296163153775\n",
      "    val_accuracy   : 0.9873670212765957\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.020119458760832265\n",
      "    accuracy       : 0.9940202902843602\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.034230964362343894\n",
      "    val_accuracy   : 0.9876994680851063\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.019967575878361725\n",
      "    accuracy       : 0.9941498815165877\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03418086285385838\n",
      "    val_accuracy   : 0.9878181990881458\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.019861617592335522\n",
      "    accuracy       : 0.9942953410629655\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03408206337468421\n",
      "    val_accuracy   : 0.9880081686930091\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.019783937999861695\n",
      "    accuracy       : 0.9943482354434664\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.034028915188057306\n",
      "    val_accuracy   : 0.9880081686930091\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.019715819396939286\n",
      "    accuracy       : 0.994427577014218\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03408890427585612\n",
      "    val_accuracy   : 0.9879606762917933\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9880 | acc@5:1.0000\n",
      "Warning: number of elements 99 is less than number of clusters. using 6 bits for quantization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.01994744734645499\n",
      "    accuracy       : 0.994168394549763\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03377086035610038\n",
      "    val_accuracy   : 0.9884830927051671\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.01976646735747808\n",
      "    accuracy       : 0.9942054206161137\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03356401160597167\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.019635358393872024\n",
      "    accuracy       : 0.994165749830738\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03346847515166836\n",
      "    val_accuracy   : 0.9886968085106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.01952633568636358\n",
      "    accuracy       : 0.9943164988151659\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033398465595227925\n",
      "    val_accuracy   : 0.988530585106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.0194539165364852\n",
      "    accuracy       : 0.9943879062288423\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03346986553948769\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.019405263270002072\n",
      "    accuracy       : 0.994424932295193\n",
      "    topk_accuracy  : 0.9999603292146243\n",
      "    val_loss       : 0.03344706613372298\n",
      "    val_accuracy   : 0.9884593465045592\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.019338586135801607\n",
      "    accuracy       : 0.994372037914692\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03338638660398886\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.01930413870703676\n",
      "    accuracy       : 0.994372037914692\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033390512244102166\n",
      "    val_accuracy   : 0.9884830927051671\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.019275087065403283\n",
      "    accuracy       : 0.9943879062288423\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033374713902222984\n",
      "    val_accuracy   : 0.988530585106383\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9885 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.31640530458841276\n",
      "    accuracy       : 0.9206187584631008\n",
      "    topk_accuracy  : 0.9982941562288423\n",
      "    val_loss       : 0.25081841362283586\n",
      "    val_accuracy   : 0.9458586626139819\n",
      "    val_topk_accuracy: 0.9991688829787234\n",
      "    epoch          : 2\n",
      "    loss           : 0.24477630193340835\n",
      "    accuracy       : 0.9437574052132701\n",
      "    topk_accuracy  : 0.9990928613744076\n",
      "    val_loss       : 0.23640562118367947\n",
      "    val_accuracy   : 0.9459061550151976\n",
      "    val_topk_accuracy: 0.9991688829787234\n",
      "    epoch          : 3\n",
      "    loss           : 0.23425858969230787\n",
      "    accuracy       : 0.9450665411306702\n",
      "    topk_accuracy  : 0.9990531905890319\n",
      "    val_loss       : 0.22730994605003518\n",
      "    val_accuracy   : 0.9468797492401216\n",
      "    val_topk_accuracy: 0.9990026595744681\n",
      "    epoch          : 4\n",
      "    loss           : 0.22623016084963676\n",
      "    accuracy       : 0.9465237813134733\n",
      "    topk_accuracy  : 0.9991298874407583\n",
      "    val_loss       : 0.2203861173797161\n",
      "    val_accuracy   : 0.9486844604863223\n",
      "    val_topk_accuracy: 0.9990026595744681\n",
      "    epoch          : 5\n",
      "    loss           : 0.21967438149367463\n",
      "    accuracy       : 0.9475896030805687\n",
      "    topk_accuracy  : 0.999166913507109\n",
      "    val_loss       : 0.21443893395839853\n",
      "    val_accuracy   : 0.9497292933130699\n",
      "    val_topk_accuracy: 0.9990026595744681\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.2141274588968234\n",
      "    accuracy       : 0.9487744372037915\n",
      "    topk_accuracy  : 0.9991854265402843\n",
      "    val_loss       : 0.20942175515154574\n",
      "    val_accuracy   : 0.9505604103343465\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 7\n",
      "    loss           : 0.2094269619451315\n",
      "    accuracy       : 0.9496762863913337\n",
      "    topk_accuracy  : 0.9991854265402843\n",
      "    val_loss       : 0.20537908406967811\n",
      "    val_accuracy   : 0.9513440349544073\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 8\n",
      "    loss           : 0.20534109827395863\n",
      "    accuracy       : 0.9502634140148951\n",
      "    topk_accuracy  : 0.9991484004739336\n",
      "    val_loss       : 0.2016826183237928\n",
      "    val_accuracy   : 0.952151405775076\n",
      "    val_topk_accuracy: 0.9993351063829787\n",
      "    epoch          : 9\n",
      "    loss           : 0.20175496977864282\n",
      "    accuracy       : 0.9508690546716316\n",
      "    topk_accuracy  : 0.9991457557549086\n",
      "    val_loss       : 0.19846813127081445\n",
      "    val_accuracy   : 0.9530062689969605\n",
      "    val_topk_accuracy: 0.9991688829787234\n",
      "    epoch          : 10\n",
      "    loss           : 0.19855867031414362\n",
      "    accuracy       : 0.9515170108327691\n",
      "    topk_accuracy  : 0.9991087296885579\n",
      "    val_loss       : 0.1955246938035843\n",
      "    val_accuracy   : 0.9541223404255319\n",
      "    val_topk_accuracy: 0.9991688829787234\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Tested model after quantization - acc@1:0.9541 | acc@5:0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.11325359007705586\n",
      "    accuracy       : 0.9659280848002708\n",
      "    topk_accuracy  : 0.9997037914691943\n",
      "    val_loss       : 0.08691241489128863\n",
      "    val_accuracy   : 0.9728580927051671\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "    epoch          : 2\n",
      "    loss           : 0.06936800628635698\n",
      "    accuracy       : 0.9784693424170616\n",
      "    topk_accuracy  : 0.9998333827014217\n",
      "    val_loss       : 0.07028206758835215\n",
      "    val_accuracy   : 0.9782247340425532\n",
      "    val_topk_accuracy: 0.9996438069908815\n",
      "    epoch          : 3\n",
      "    loss           : 0.059345069175842105\n",
      "    accuracy       : 0.9814234935680433\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.0642540122758835\n",
      "    val_accuracy   : 0.9804806231003039\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "    epoch          : 4\n",
      "    loss           : 0.05553889097822433\n",
      "    accuracy       : 0.9827194058903181\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.06189433553632587\n",
      "    val_accuracy   : 0.9818341565349543\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "    epoch          : 5\n",
      "    loss           : 0.05382688093038885\n",
      "    accuracy       : 0.9831452056533514\n",
      "    topk_accuracy  : 0.9998704087677726\n",
      "    val_loss       : 0.0607602161533655\n",
      "    val_accuracy   : 0.9821191109422492\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.0529429853995335\n",
      "    accuracy       : 0.983388519803656\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.06015785648784739\n",
      "    val_accuracy   : 0.9824990501519756\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "    epoch          : 7\n",
      "    loss           : 0.05245752468422691\n",
      "    accuracy       : 0.9833673620514556\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.05998735870928206\n",
      "    val_accuracy   : 0.982641527355623\n",
      "    val_topk_accuracy: 0.9996438069908815\n",
      "    epoch          : 8\n",
      "    loss           : 0.052122358806537226\n",
      "    accuracy       : 0.9833700067704807\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.05951671611438406\n",
      "    val_accuracy   : 0.9828789893617021\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "    epoch          : 9\n",
      "    loss           : 0.051877187869524784\n",
      "    accuracy       : 0.9833567831753555\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.059404127002555006\n",
      "    val_accuracy   : 0.9828314969604862\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "    epoch          : 10\n",
      "    loss           : 0.05169052795189216\n",
      "    accuracy       : 0.9834043881178064\n",
      "    topk_accuracy  : 0.9998889218009479\n",
      "    val_loss       : 0.05922692286920674\n",
      "    val_accuracy   : 0.9828552431610943\n",
      "    val_topk_accuracy: 0.9996675531914894\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Tested model after quantization - acc@1:0.9828 | acc@5:0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.027547983907580252\n",
      "    accuracy       : 0.9915368991198376\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.04049808491873456\n",
      "    val_accuracy   : 0.986322188449848\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 2\n",
      "    loss           : 0.026987186020076874\n",
      "    accuracy       : 0.9915924382193636\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.04027068435878275\n",
      "    val_accuracy   : 0.9864884118541033\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 3\n",
      "    loss           : 0.026628058281521892\n",
      "    accuracy       : 0.9918145946174679\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.040279154759891175\n",
      "    val_accuracy   : 0.9872720364741642\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 4\n",
      "    loss           : 0.026379139853091867\n",
      "    accuracy       : 0.991928317535545\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.04019442576162042\n",
      "    val_accuracy   : 0.9873195288753799\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 5\n",
      "    loss           : 0.026218272634711274\n",
      "    accuracy       : 0.9920737770819229\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.040122833915371844\n",
      "    val_accuracy   : 0.9876282294832827\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.026127617538882358\n",
      "    accuracy       : 0.9921213820243737\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.039955653904124776\n",
      "    val_accuracy   : 0.9873670212765957\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 7\n",
      "    loss           : 0.026011050485959496\n",
      "    accuracy       : 0.9920949348341233\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.040032709621764875\n",
      "    val_accuracy   : 0.9874857522796352\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 8\n",
      "    loss           : 0.025955863907200536\n",
      "    accuracy       : 0.9920367510155721\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.040053883517913995\n",
      "    val_accuracy   : 0.9875094984802432\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 9\n",
      "    loss           : 0.02589900422205716\n",
      "    accuracy       : 0.9920182379823967\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.04023102395474277\n",
      "    val_accuracy   : 0.9874620060790273\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "    epoch          : 10\n",
      "    loss           : 0.025859153817652757\n",
      "    accuracy       : 0.9920579087677726\n",
      "    topk_accuracy  : 0.999944460900474\n",
      "    val_loss       : 0.04011592633546667\n",
      "    val_accuracy   : 0.9873195288753799\n",
      "    val_topk_accuracy: 0.9998337765957447\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Tested model after quantization - acc@1:0.9873 | acc@5:0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.022415817798263175\n",
      "    accuracy       : 0.9932004273865944\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.035730773870377465\n",
      "    val_accuracy   : 0.9886968085106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.022042726063648443\n",
      "    accuracy       : 0.9932956372714963\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0355999270711649\n",
      "    val_accuracy   : 0.9889580167173252\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.02177613552867201\n",
      "    accuracy       : 0.9933167950236966\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.035220385301224094\n",
      "    val_accuracy   : 0.9893379559270518\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.02159395780369101\n",
      "    accuracy       : 0.9933511763710223\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0349908353998623\n",
      "    val_accuracy   : 0.9891954787234043\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.021477339860168837\n",
      "    accuracy       : 0.9933882024373731\n",
      "    topk_accuracy  : 0.9999603292146243\n",
      "    val_loss       : 0.034857210237532854\n",
      "    val_accuracy   : 0.9893617021276596\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.021360924856415046\n",
      "    accuracy       : 0.9934278732227488\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03487996851153513\n",
      "    val_accuracy   : 0.9893142097264437\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.0212874089114443\n",
      "    accuracy       : 0.9935759774881516\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03486110060971151\n",
      "    val_accuracy   : 0.9893142097264437\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.021241872120920074\n",
      "    accuracy       : 0.9935892010832769\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03475953390861445\n",
      "    val_accuracy   : 0.9893379559270518\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.02119156061335118\n",
      "    accuracy       : 0.9936315165876777\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03470970645665805\n",
      "    val_accuracy   : 0.9896941489361702\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.021162322437406133\n",
      "    accuracy       : 0.9935733327691266\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03479304125017308\n",
      "    val_accuracy   : 0.9896466565349543\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch10.pth ...\n",
      "Tested model after quantization - acc@1:0.9897 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.020471413895130262\n",
      "    accuracy       : 0.993851028266757\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03428098182887473\n",
      "    val_accuracy   : 0.989029255319149\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.020254488155368264\n",
      "    accuracy       : 0.9939409487136086\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03428379007852934\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.02007716753899663\n",
      "    accuracy       : 0.9940758293838863\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.034145193536111965\n",
      "    val_accuracy   : 0.9888630319148937\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.019951972647672695\n",
      "    accuracy       : 0.9940890529790114\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03418289988915971\n",
      "    val_accuracy   : 0.988981762917933\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.01984811422246248\n",
      "    accuracy       : 0.994168394549763\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03415422317908799\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.01977072043318999\n",
      "    accuracy       : 0.9941869075829384\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03409596245260315\n",
      "    val_accuracy   : 0.9888630319148937\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.01970988412603396\n",
      "    accuracy       : 0.9941498815165877\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03416091475160198\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9888 | acc@5:1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.01999449993812554\n",
      "    accuracy       : 0.994369393195667\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03388972043931643\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.019822194275194646\n",
      "    accuracy       : 0.9943879062288423\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03386649234775216\n",
      "    val_accuracy   : 0.9884593465045592\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.019695276465108044\n",
      "    accuracy       : 0.9944090639810427\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03384553635136244\n",
      "    val_accuracy   : 0.9885780775075989\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.0196055055533037\n",
      "    accuracy       : 0.9945016291469194\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03365854302698627\n",
      "    val_accuracy   : 0.9888630319148937\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.019539122568759008\n",
      "    accuracy       : 0.9945201421800948\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03373328197816823\n",
      "    val_accuracy   : 0.9888392857142858\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.01949348537057144\n",
      "    accuracy       : 0.9945016291469194\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0336573857546566\n",
      "    val_accuracy   : 0.988530585106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.019456755185623373\n",
      "    accuracy       : 0.9945386552132701\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03371429244214867\n",
      "    val_accuracy   : 0.9885068389057751\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.01942714361249915\n",
      "    accuracy       : 0.9945941943127962\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03365538635865805\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.019409743310901298\n",
      "    accuracy       : 0.9946285756601219\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033742634364047105\n",
      "    val_accuracy   : 0.9886493161094224\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.019389347332448067\n",
      "    accuracy       : 0.994631220379147\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03366712400531198\n",
      "    val_accuracy   : 0.9886968085106383\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9887 | acc@5:1.0000\n",
      "Warning: number of elements 99 is less than number of clusters. using 6 bits for quantization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n",
      "C:\\Users\\Dario\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1037: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  \"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch          : 1\n",
      "    loss           : 0.01991460572291492\n",
      "    accuracy       : 0.9942054206161137\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03362055638547432\n",
      "    val_accuracy   : 0.9883643617021277\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 2\n",
      "    loss           : 0.019755609220961964\n",
      "    accuracy       : 0.9942794727488151\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03356549009046656\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 3\n",
      "    loss           : 0.019633700004370522\n",
      "    accuracy       : 0.9942609597156398\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03346851462389323\n",
      "    val_accuracy   : 0.9888630319148937\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 4\n",
      "    loss           : 0.019589557620846795\n",
      "    accuracy       : 0.994274183310765\n",
      "    topk_accuracy  : 0.9999603292146243\n",
      "    val_loss       : 0.03345125477186384\n",
      "    val_accuracy   : 0.9890055091185411\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 5\n",
      "    loss           : 0.019477625354579364\n",
      "    accuracy       : 0.9943508801624916\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03359852599951022\n",
      "    val_accuracy   : 0.9886255699088146\n",
      "    val_topk_accuracy: 1.0\n",
      "Saving checkpoint: runs\\models\\Mnist_LeNet5\\0313_105747\\checkpoint-epoch5.pth ...\n",
      "    epoch          : 6\n",
      "    loss           : 0.01941399218986844\n",
      "    accuracy       : 0.9943905509478673\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03342341140477344\n",
      "    val_accuracy   : 0.9886968085106383\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 7\n",
      "    loss           : 0.019374353345759408\n",
      "    accuracy       : 0.9944064192620177\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03347377210330377\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 8\n",
      "    loss           : 0.019339213589696275\n",
      "    accuracy       : 0.9944619583615437\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.03343284586840804\n",
      "    val_accuracy   : 0.9886730623100305\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 9\n",
      "    loss           : 0.019313333897026018\n",
      "    accuracy       : 0.9944831161137441\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.0334987990444209\n",
      "    val_accuracy   : 0.9888155395136777\n",
      "    val_topk_accuracy: 1.0\n",
      "    epoch          : 10\n",
      "    loss           : 0.019291928084898227\n",
      "    accuracy       : 0.994427577014218\n",
      "    topk_accuracy  : 0.9999629739336493\n",
      "    val_loss       : 0.033442451209919725\n",
      "    val_accuracy   : 0.9888630319148937\n",
      "    val_topk_accuracy: 1.0\n",
      "Validation performance didn't improve for 5 epochs. Training stops.\n",
      "Tested model after quantization - acc@1:0.9888 | acc@5:1.0000\n"
     ]
    }
   ],
   "source": [
    "from trainer.compression_trainer import CompressionTrainer\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "train = False\n",
    "\n",
    "fns = ['linear_quantization','forgy_quantization','density_quantization']\n",
    "bits = 8\n",
    "\n",
    "sensitivities = OrderedDict()\n",
    "\n",
    "# build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = config.init_obj('optimizer', torch.optim, trainable_params)\n",
    "lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)\n",
    "\n",
    "trainer = CompressionTrainer(model, criterion, metrics, optimizer,\n",
    "                  config=config,\n",
    "                  device=device,\n",
    "                  data_loader=data_loader,\n",
    "                  valid_data_loader=valid_data_loader,\n",
    "                  lr_scheduler=lr_scheduler)\n",
    "\n",
    "trainer.prune()\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "# # avoid model reinitialization inside trainer\n",
    "config.resume = None\n",
    "\n",
    "for fn in fns:\n",
    "    sensitivity = OrderedDict()\n",
    "    for b in range(1,bits):\n",
    "        config['quantizer']['type'] = fn\n",
    "\n",
    "        config['quantizer']['levels'] = {k:b for k in config['quantizer']['levels'].keys()}\n",
    "\n",
    "        # Make a copy of the model, because when we apply the zeros mask (i.e.\n",
    "        # perform pruning), the model's weights are altered\n",
    "        model_cpy = deepcopy(model)\n",
    "\n",
    "        trainable_params = filter(lambda p: p.requires_grad, model_cpy.parameters())\n",
    "        optimizer = config.init_obj('optimizer', torch.optim, trainable_params)\n",
    "        lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)\n",
    "\n",
    "        trainer = CompressionTrainer(model_cpy, criterion, metrics, optimizer,\n",
    "                          config=config,\n",
    "                          device=device,\n",
    "                          data_loader=data_loader,\n",
    "                          valid_data_loader=valid_data_loader,\n",
    "                          lr_scheduler=lr_scheduler)\n",
    "\n",
    "        trainer.quantize()\n",
    "\n",
    "        _, acc1,acc5 = trainer._valid_epoch(-1).values()\n",
    "\n",
    "        sensitivity[b] = [acc1, acc5]\n",
    "    sensitivities[fn] = sensitivity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_quantization\n",
      "[0.9532912234042553, 0.982689019756839, 0.9889580167173252, 0.9891717325227964, 0.9891717325227964, 0.9891954787234043, 0.9882693768996961] [1, 2, 3, 4, 5, 6, 7]\n",
      "forgy_quantization\n",
      "[0.9535524316109423, 0.9818579027355624, 0.9876519756838905, 0.9876282294832827, 0.9881981382978723, 0.9879844224924011, 0.9884830927051671] [1, 2, 3, 4, 5, 6, 7]\n",
      "density_quantization\n",
      "[0.9541698328267478, 0.9828552431610943, 0.9873195288753799, 0.9896941489361702, 0.9888392857142858, 0.9886968085106383, 0.9888155395136777] [1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for param_name, sensitivity in sensitivities.items():\n",
    "    print(param_name)\n",
    "    sense = [values[0] for sparsity, values in sensitivity.items()]\n",
    "    sparsities = [sparsity for sparsity, values in sensitivity.items()]\n",
    "    print(sense,sparsities)\n",
    "    plt.plot(sparsities, sense, label=param_name.replace('_',' '))\n",
    "\n",
    "plt.ylabel('top1 Accuracy')\n",
    "plt.xlabel('index bits')\n",
    "plt.title('Quantization')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower left',\n",
    "           ncol=2, borderaxespad=0.)\n",
    "fig.savefig('quantization-comparison.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def get_param_names(df):\n",
    "    return list(set(df['parameter']))\n",
    "\n",
    "def get_sensitivity_levels(df):\n",
    "    return list(set(df['sparsity']))\n",
    "\n",
    "def df2sensitivities(df):\n",
    "    param_names = get_param_names(df)\n",
    "    sparsities = get_sensitivity_levels(df)\n",
    "\n",
    "    sensitivities = {}\n",
    "    for param_name in param_names:\n",
    "        sensitivities[param_name] = OrderedDict()\n",
    "        param_stats = df[(df.parameter == param_name)]\n",
    "\n",
    "        for row in range(len(param_stats.index)):\n",
    "            s = param_stats.iloc[[row]].sparsity\n",
    "\n",
    "            loss = param_stats.iloc[[row]].loss\n",
    "            top1 = param_stats.iloc[[row]].top1\n",
    "            top5 = param_stats.iloc[[row]].top5\n",
    "            sensitivities[param_name][float(s)] = {'loss':float(loss), 'val_accuracy':float(top1), 'val_topk_accuracy':float(top5),}\n",
    "    return sensitivities\n",
    "\n",
    "def view2(level, acc):\n",
    "    filtered = df[df.sparsity == level]\n",
    "    s = filtered.style.apply(highlight_min_max)\n",
    "\n",
    "    param_names = filtered['parameter']\n",
    "\n",
    "    # Plot the sensitivities\n",
    "    x = range(filtered[acc].shape[0])\n",
    "    y = filtered[acc].values.tolist()\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.plot(x, y, label=param_names, marker=\"o\", markersize=10, markerfacecolor=\"C1\")\n",
    "    plt.ylabel(str(acc))\n",
    "    plt.xlabel('parameter')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.xticks(x, param_names)\n",
    "    plt.title('Pruning Sensitivity per layer %d' % level)\n",
    "    #return s\n",
    "\n",
    "def highlight_min_max(s):\n",
    "    \"\"\"Highlight the max and min values in the series\"\"\"\n",
    "    if s.name not in ['top1', 'top5']:\n",
    "        return ['' for v in s]\n",
    "\n",
    "    is_max = s == s.max()\n",
    "    maxes = ['background-color: green' if v else '' for v in is_max]\n",
    "    is_min = s == s.min()\n",
    "    mins = ['background-color: red' if v else '' for v in is_min]\n",
    "    return [h1 if len(h1)>len(h2) else h2 for (h1,h2) in zip(maxes, mins)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import make_paths_relative_to_root\n",
    "import matplotlib.pyplot as plt\n",
    "from sensitivity import plot_sensitivities\n",
    "\n",
    "make_paths_relative_to_root()\n",
    "\n",
    "# df = pd.read_csv('runs/Imagenette_AlexNet/0320_234640/in_alexnet_sensitivity_analysis_notrain.csv')\n",
    "# df = pd.read_csv('runs/Mnist_LeNet5/0321_095114/mnist_lenet5sensitivity_analysis.csv')\n",
    "df = pd.read_csv('runs/Mnist_LeNet300/sensitivity-analysis/mnist_sensitivity_analysis.csv')\n",
    "#df = pd.read_csv('../examples/sensitivity-analysis/resnet20-cifar/sensitivity_filter_wise.csv')\n",
    "\n",
    "# df['sparsity'] = round(df['sparsity'], 2)\n",
    "\n",
    "baseline_acc = df[df['sparsity']==0.0].top1.mean()\n",
    "\n",
    "df['top1'] = (df['top1'] - baseline_acc)/df['top1'] * 100\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "sensitivities = df2sensitivities(df)\n",
    "fig = plot_sensitivities(sensitivities,'val_accuracy')\n",
    "ax = fig.axes[0]\n",
    "ax.set_xlim(0.4)\n",
    "ax.set_ylim((-30,2))\n",
    "ax.set_ylabel('top1 accuracy % difference')\n",
    "fig.savefig('lenet300_pruning_sensitivity.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
